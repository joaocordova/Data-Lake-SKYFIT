# SkyFit Data Lake - Architecture

## Overview

The SkyFit Data Lake follows the **Medallion Architecture** pattern, a data design pattern used to logically organize data in a lakehouse with the goal of incrementally improving the quality and structure of data as it flows through each layer.

## Layers

### ðŸ¥‰ Bronze Layer (Raw)

**Location**: Azure Data Lake Storage Gen2

**Purpose**: Store raw, immutable data exactly as received from source systems.

**Characteristics**:
- Append-only (never modified)
- Compressed (.jsonl.gz)
- Partitioned by source, scope, entity, date, run_id
- Full audit trail

**Path Pattern**:
```
bronze/{source}/scope={scope}/entity={entity}/ingestion_date={YYYY-MM-DD}/run_id={timestamp}/part-{N}.jsonl.gz
```

### ðŸ¥ˆ Silver Layer (Staging)

**Location**: PostgreSQL - `stg_*` schemas

**Purpose**: Validated, deduplicated data with full lineage tracking.

**Characteristics**:
- JSONB format (schema-flexible)
- Full lineage (blob_path, line_no)
- Deduplication via UPSERT
- Queryable for debugging

**Tables**:
- `stg_pipedrive.deals_raw`
- `stg_pipedrive.persons_raw`
- `stg_zendesk.tickets_raw`
- etc.

### ðŸ¥‡ Gold Layer (Core)

**Location**: PostgreSQL - `core` schema

**Purpose**: Analytics-ready, strongly-typed dimensional model.

**Characteristics**:
- Normalized star schema
- Proper data types
- Indexed for performance
- ML-ready

**Tables**:
- `core.pd_deals` (Fact)
- `core.pd_activities` (Fact)
- `core.zd_tickets` (Fact)
- Dimension tables (users, organizations, etc.)

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Source    â”‚     â”‚   Bronze    â”‚     â”‚   Silver    â”‚     â”‚    Gold     â”‚
â”‚    APIs     â”‚â”€â”€â”€â”€â–ºâ”‚   (ADLS)    â”‚â”€â”€â”€â”€â–ºâ”‚   (STG)     â”‚â”€â”€â”€â”€â–ºâ”‚   (CORE)    â”‚
â”‚             â”‚     â”‚   .jsonl.gz â”‚     â”‚   JSONB     â”‚     â”‚   Typed     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    
     Extract          Watermark-based      UPSERT with        JSONB extraction
     via API          incremental          lineage            + type casting
```

## Key Components

### Extractors (Bronze)

| Extractor | Source | Entities |
|-----------|--------|----------|
| `pipedrive_bronze.py` | Pipedrive API | deals, persons, activities, organizations, pipelines, stages, users |
| `zendesk_bronze.py` | Zendesk API | tickets, users, organizations, groups, ticket_fields, ticket_forms |

### Loaders (Silver)

| Loader | Target Schema | Pattern |
|--------|---------------|---------|
| `load_pipedrive_stg.py` | `stg_pipedrive` | UPSERT by (scope, blob_path, line_no) |
| `load_zendesk_stg.py` | `stg_zendesk` | UPSERT by (blob_path, line_no) |

### Transformers (Gold)

| Transformer | Target Tables | Pattern |
|-------------|---------------|---------|
| `normalize_pipedrive.py` | `core.pd_*` | JSONB extraction + UPSERT by business key |
| `normalize_zendesk.py` | `core.zd_*` | JSONB extraction + UPSERT by business key |

## Architecture Decision Records

| ADR | Title | Status |
|-----|-------|--------|
| [ADR-001](decisions/ADR-001-postgresql-as-warehouse.md) | PostgreSQL as Data Warehouse | Accepted |
| [ADR-002](decisions/ADR-002-jsonb-staging-layer.md) | JSONB Staging Layer | Accepted |
| [ADR-003](decisions/ADR-003-multi-tenant-single-table.md) | Multi-Tenant Single Table Design | Accepted |

## Infrastructure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Azure Cloud                               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Storage Account     â”‚    â”‚  PostgreSQL Flexible â”‚          â”‚
â”‚  â”‚  (ADLS Gen2)         â”‚    â”‚  Server              â”‚          â”‚
â”‚  â”‚                      â”‚    â”‚                      â”‚          â”‚
â”‚  â”‚  Container: datalake â”‚    â”‚  Database: skyfitevo â”‚          â”‚
â”‚  â”‚  Tier: Cool          â”‚    â”‚  SKU: Burstable B1ms â”‚          â”‚
â”‚  â”‚                      â”‚    â”‚  Storage: 32GB       â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ VPN/Private Endpoint
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     On-Premises / Local                          â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Windows Server      â”‚    â”‚  Power BI Desktop    â”‚          â”‚
â”‚  â”‚                      â”‚    â”‚                      â”‚          â”‚
â”‚  â”‚  - Python 3.10+      â”‚    â”‚  DirectQuery to      â”‚          â”‚
â”‚  â”‚  - Task Scheduler    â”‚    â”‚  PostgreSQL          â”‚          â”‚
â”‚  â”‚  - Pipeline scripts  â”‚    â”‚                      â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Security

| Component | Security Measure |
|-----------|------------------|
| ADLS | Access Key (rotate quarterly) |
| PostgreSQL | SSL required, password auth |
| API Tokens | Environment variables (.env) |
| Credentials | Never committed to git |

## Monitoring

| Metric | Location | Alert Threshold |
|--------|----------|-----------------|
| Pipeline success | `logs/daily_pipeline_*.log` | Any failure |
| Data freshness | `core.*._updated_at` | > 24h stale |
| Record counts | `health_check.ps1` output | > 10% variance |
| Duplicates | Health check queries | Any duplicates |

## Future Architecture

When scale requires, migration path to:

```
Current                              Future
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Windows Task Scheduler          â”€â”€â–º  Apache Airflow / Azure Data Factory
PostgreSQL                      â”€â”€â–º  Snowflake / Databricks
Python transforms               â”€â”€â–º  dbt + Python models  
Local execution                 â”€â”€â–º  Kubernetes / Azure Container Apps
Manual monitoring               â”€â”€â–º  DataDog / Azure Monitor
```
